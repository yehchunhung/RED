{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoBERT\n",
    "The notebook is for detecting emotion on all the utterances in the dialogue dataset. The classifier is built with BERT, a language model mainly used in natural language processing. More details can be seen [HERE](https://github.com/anuradha1992/EmpatheticIntents.git)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMyWdJVmhmLw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XE_6nvgmhxoB",
    "outputId": "0add01dd-171c-49c0-c42d-757246a74a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# After eager execution is enabled, operations are executed as they are\n",
    "# defined and Tensor objects hold concrete values, which can be accessed as\n",
    "# numpy.ndarray`s through the numpy() method.\n",
    "assert tf.multiply(6, 7).numpy() == 42\n",
    "\n",
    "print(tf.multiply(6, 7).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GVtBwetM4Yr",
    "outputId": "9a43f060-37ef-49e6-81aa-9a1d34b5cb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "WARNING:tensorflow:From <ipython-input-3-e50853fad5b2>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8vix6fNiKcA"
   },
   "outputs": [],
   "source": [
    "emotions = ['afraid',\n",
    "            'angry',\n",
    "            'annoyed',\n",
    "            'anticipating',\n",
    "            'anxious',\n",
    "            'apprehensive',\n",
    "            'ashamed',\n",
    "            'caring',\n",
    "            'confident',\n",
    "            'content',\n",
    "            'devastated',\n",
    "            'disappointed',\n",
    "            'disgusted',\n",
    "            'embarrassed',\n",
    "            'excited',\n",
    "            'faithful',\n",
    "            'furious',\n",
    "            'grateful',\n",
    "            'guilty',\n",
    "            'hopeful',\n",
    "            'impressed',\n",
    "            'jealous',\n",
    "            'joyful',\n",
    "            'lonely',\n",
    "            'nostalgic',\n",
    "            'prepared',\n",
    "            'proud',\n",
    "            'sad',\n",
    "            'sentimental',\n",
    "            'surprised',\n",
    "            'terrified',\n",
    "            'trusting']\n",
    "\n",
    "ED_emotions = ['afraid', 'angry','annoyed',\n",
    "            'anticipating','anxious','apprehensive','ashamed','caring','confident','content','devastated','disappointed',\n",
    "            'disgusted','embarrassed','excited','faithful','furious','grateful','guilty','hopeful','impressed','jealous',\n",
    "            'joyful','lonely','nostalgic','prepared','proud','sad','sentimental','surprised','terrified','trusting',\n",
    "            'agreeing','acknowledging','encouraging','consoling','sympathizing','suggesting','questioning','wishing','neutral']\n",
    "\n",
    "path = '/content/gdrive/My Drive/Colab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYoHYbd6ijg0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate, name = 'multi_head_attention'):\n",
    "        super().__init__(name = name)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, name = 'query')\n",
    "        self.wk = tf.keras.layers.Dense(d_model, name = 'key')\n",
    "        self.wv = tf.keras.layers.Dense(d_model, name = 'value')\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate, name = 'mha_dropout')\n",
    "        self.dense = tf.keras.layers.Dense(d_model, name = 'mha_output')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead) \n",
    "        but it must be broadcastable for addition.\n",
    "        Args:\n",
    "            q: query shape == (..., seq_len_q, depth)\n",
    "            k: key shape == (..., seq_len_k, depth)\n",
    "            v: value shape == (..., seq_len_v, depth_v)\n",
    "            mask: Float tensor with shape broadcastable \n",
    "                to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        # (As claimed in the RoBERTa implementation.)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + tf.math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * tf.math.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + tf.math.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "act_funcs = {'gelu': gelu, 'relu': tf.nn.relu}\n",
    "\n",
    "# Pointwise Feed Forward Network\n",
    "def point_wise_feed_forward_network(d_model, dff, hidden_act):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation = act_funcs[hidden_act],\n",
    "            name = 'ff_hidden'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model, name = 'ff_output')  # (batch_size, seq_len, d_model)\n",
    "    ], name = 'ff_network')\n",
    "\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, hidden_act, dropout_rate, layer_norm_eps, layer_num):\n",
    "        super().__init__(name = 'encoder_layer_{:02d}'.format(layer_num))\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff, hidden_act)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_1')\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_2')\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_1')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_2')\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-IBYR3HinCB"
   },
   "outputs": [],
   "source": [
    "def loss_function(real_emot, pred_emot):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True, reduction = 'none')\n",
    "    loss_ = scce(real_emot, pred_emot)\n",
    "    return loss_\n",
    "\n",
    "class EmoBERT(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, hidden_act, dropout_rate,\n",
    "                 layer_norm_eps, max_position_embed, vocab_size, num_emotions):\n",
    "        super().__init__(name = 'emo_bert')\n",
    "\n",
    "        self.padding_idx = 1\n",
    "\n",
    "        # Embedding layers\n",
    "        self.word_embeddings = tf.keras.layers.Embedding(vocab_size, d_model, name = 'word_embed')\n",
    "        self.pos_embeddings = tf.keras.layers.Embedding(max_position_embed, d_model, name = 'pos_embed')\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_embed')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_embed')\n",
    "\n",
    "        # Encoder layers\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, hidden_act, dropout_rate, layer_norm_eps, i)\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layers\n",
    "        self.attention_v = tf.keras.layers.Dense(1, use_bias = False, name = 'attention_v')\n",
    "        self.attention_layer = tf.keras.layers.Dense(d_model, activation = 'tanh', name = 'attention_layer')\n",
    "        self.hidden_layer = tf.keras.layers.Dense(d_model, activation = 'tanh', name = 'hidden_layer')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_emotions, name = 'output_layer')\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape == (batch_size, seq_len)\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Add word embedding and position embedding.\n",
    "        pos = tf.range(self.padding_idx + 1, seq_len + self.padding_idx + 1)\n",
    "        pos = tf.broadcast_to(pos, tf.shape(x))\n",
    "        x = self.word_embeddings(x)  # (batch_size, seq_len, d_model)\n",
    "        x += self.pos_embeddings(pos)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        # x.shape == (batch_size, seq_len, d_model)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        projected = self.attention_layer(x)  # (batch_size, seq_len, d_model)\n",
    "        logits = tf.squeeze(self.attention_v(projected), 2)  # (batch_size, seq_len)\n",
    "        logits += (tf.squeeze(mask) * -1e9)  # Mask out the padding positions\n",
    "        scores = tf.expand_dims(tf.nn.softmax(logits), 1)  # (batch_size, 1, seq_len)\n",
    "\n",
    "        # x.shape == (batch_size, d_model)\n",
    "        x = tf.squeeze(tf.matmul(scores, x), 1)\n",
    "\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x  # (batch_size, num_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjnLowEgip_u"
   },
   "outputs": [],
   "source": [
    "# Masking\n",
    "def create_padding_mask(seq):\n",
    "    # To be consistent with RoBERTa, the padding index is set to 1.\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # Add extra dimensions so that we can add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_masks(inp):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    return enc_padding_mask\n",
    "\n",
    "def build_model(model, max_length, vocab_size):\n",
    "    inp = np.ones((1, max_length), dtype = np.int32)\n",
    "    inp[0,:max_length//2] = np.random.randint(2, vocab_size, size = max_length//2)\n",
    "    inp = tf.constant(inp)\n",
    "    enc_padding_mask = create_masks(inp)\n",
    "    _ = model(inp, True, enc_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opNb8cZUisvC"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, peak_lr, total_steps, warmup_steps):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.peak_lr = peak_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step / self.warmup_steps\n",
    "        arg2 = (self.total_steps - step) / (self.total_steps - self.warmup_steps)\n",
    "        return self.peak_lr * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vphv5fJaivkh",
    "outputId": "bbc6da7f-f410-4d65-f3c3-4c81380becd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 13.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/78/4067ce89180daf0b2027df4b3e4c4734d73b99c3a664d262a4c4d5ac1021/boto3-1.16.47-py2.py3-none-any.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 26.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.19.4)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 27.0MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
      "\u001b[?25hCollecting botocore<1.20.0,>=1.19.47\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/4a/16ffdfc33d93f02604ae9ed1ddb6369030b6f61b583f31dc84e0d0da05c1/botocore-1.19.47-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2MB 48.2MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.47->boto3->pytorch-transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ec515a8db7520f54e44c4cb0c6ee90b4fbbfcddb381747ee75087dc092dd614c\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "\u001b[31mERROR: botocore 1.19.47 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, pytorch-transformers\n",
      "Successfully installed boto3-1.16.47 botocore-1.19.47 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAvn21Cvix75",
    "outputId": "94adcf4c-d820-4458-f455-94a38fad8c98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:00<00:00, 1736669.87B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1319010.10B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emobert-checkpoints/ckpt-10\n",
      "emobert-checkpoints/ckpt-5\n",
      "Checkpoint at epoch 5 restored!!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pytorch_transformers import RobertaTokenizer\n",
    "\n",
    "num_layers = 12\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "dff = d_model * 4\n",
    "hidden_act = 'gelu'  # Use 'gelu' or 'relu'\n",
    "dropout_rate = 0.1\n",
    "layer_norm_eps = 1e-5\n",
    "max_position_embed = 514\n",
    "num_emotions = 41  # Number of emotion categories\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "max_length = 100  # Maximum number of tokens\n",
    "buffer_size = 100000\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "peak_lr = 2e-5\n",
    "total_steps = 7000\n",
    "warmup_steps = 700\n",
    "adam_beta_1 = 0.9\n",
    "adam_beta_2 = 0.98\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "checkpoint_path = 'emobert-checkpoints'  # Need to replace this with correct checkpoint path\n",
    "\n",
    "SOS_ID = tokenizer.encode('<s>')[0]\n",
    "EOS_ID = tokenizer.encode('</s>')[0]\n",
    "\n",
    "emobert = EmoBERT(num_layers, d_model, num_heads, dff, hidden_act, dropout_rate,\n",
    "            layer_norm_eps, max_position_embed, vocab_size, num_emotions)\n",
    "\n",
    "build_model(emobert, max_length, vocab_size)\n",
    "\n",
    "learning_rate = CustomSchedule(peak_lr, total_steps, warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = adam_beta_1, beta_2 = adam_beta_2,\n",
    "            epsilon = adam_epsilon)\n",
    "#train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "\n",
    "# Define the checkpoint manager.\n",
    "ckpt = tf.train.Checkpoint(model = emobert, optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = None)\n",
    "\n",
    "# If a checkpoint exists, restore the latest checkpoint.\n",
    "print(ckpt_manager.latest_checkpoint)\n",
    "#if ckpt_manager.latest_checkpoint:\n",
    "#    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#    print('Latest checkpoint restored!!')\n",
    "#    f.write('Latest checkpoint restored!!\\n')\n",
    "\n",
    "# Restore the checkpoint at epoch 8 - v1.\n",
    "# Restore the checkpoint at epoch 3 - v2.\n",
    "# Restore the checkpoint at epoch 5 - v2.\n",
    "print(ckpt_manager.checkpoints[4])\n",
    "# ckpt.restore(ckpt_manager.checkpoints[4])\n",
    "print('Checkpoint at epoch 5 restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJnVaRYxjhye"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def predict_emotion(uttrs):\n",
    "\n",
    "    bs = 1\n",
    "    \n",
    "    #with open(join(data_xpath, 'uttrs.txt'), 'r') as f:\n",
    "    #    uttrs = f.read().splitlines()\n",
    "\n",
    "    uttr_ids = np.ones((len(uttrs), max_length), dtype = np.int32)\n",
    "    #for i, u in tqdm(enumerate(uttrs), total = len(uttrs)):\n",
    "    i = 0\n",
    "    u = uttrs[0]\n",
    "    u_ids = [SOS_ID] + tokenizer.encode(u)[:(max_length-2)] + [EOS_ID]\n",
    "    uttr_ids[i, :len(u_ids)] = u_ids\n",
    "\n",
    "    uttr_emots = np.zeros((len(uttrs), num_emotions))\n",
    "    num_batches = len(uttrs) // bs\n",
    "    #for i in tqdm(range(num_batches)):\n",
    "    i = 0\n",
    "    s = i * bs\n",
    "    t = s + bs\n",
    "    inp = tf.constant(uttr_ids[s:t])\n",
    "    enc_padding_mask = create_masks(inp)\n",
    "    pred = emobert(inp, False, enc_padding_mask)\n",
    "    pred = tf.nn.softmax(pred).numpy()\n",
    "\n",
    "    return pred[0]\n",
    "    #np.save(join(data_path, 'uttr_emots.npy'), uttr_emots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_9dEKp-aTIu",
    "outputId": "91aec156-c5fd-4db7-9adc-bc0bbd389168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 've got a bad feeling about this .\n",
      "prepared, confident, agreeing, apprehensive, sad, sympathizing, anticipating, jealous, trusting, wishing, embarrassed, sentimental, disgusted, disappointed, furious, suggesting, terrified, content, encouraging, angry, impressed, devastated, anxious, ashamed, joyful, annoyed, faithful, guilty, afraid, caring, neutral, excited, \n",
      "prepared (0.1148149), confident (0.07197149), agreeing (0.046944674), apprehensive (0.04430049), sad (0.037095327), sympathizing (0.037022237), anticipating (0.0356097), jealous (0.033728626), trusting (0.028143365), wishing (0.027673488), embarrassed (0.027670458), sentimental (0.02680548), disgusted (0.026611352), disappointed (0.026011076), furious (0.025100425), suggesting (0.02333053), terrified (0.022534167), content (0.02207666), encouraging (0.021363156), angry (0.02080253), impressed (0.020738767), devastated (0.019967636), anxious (0.019266138), ashamed (0.018409066), joyful (0.018131724), annoyed (0.017553777), faithful (0.016796665), guilty (0.015990837), afraid (0.015428739), caring (0.01452031), neutral (0.014253496), excited (0.013195147), \n",
      "\n",
      "You see that dividing line ?  You dare stand near that dividing line ?\n",
      "confident, prepared, agreeing, trusting, ashamed, embarrassed, anticipating, jealous, impressed, caring, content, disappointed, afraid, devastated, anxious, encouraging, terrified, joyful, suggesting, excited, sentimental, sad, guilty, apprehensive, nostalgic, wishing, disgusted, hopeful, annoyed, grateful, consoling, neutral, \n",
      "confident (0.14611419), prepared (0.12298515), agreeing (0.06264101), trusting (0.06254435), ashamed (0.05942077), embarrassed (0.0514103), anticipating (0.040485114), jealous (0.03626432), impressed (0.03302128), caring (0.028917799), content (0.021837616), disappointed (0.021283567), afraid (0.020225026), devastated (0.019600043), anxious (0.017628463), encouraging (0.015629135), terrified (0.014246216), joyful (0.0137795275), suggesting (0.013462095), excited (0.013324186), sentimental (0.01280015), sad (0.012579256), guilty (0.011884389), apprehensive (0.011861041), nostalgic (0.01092438), wishing (0.010625684), disgusted (0.010243297), hopeful (0.01011989), annoyed (0.009333131), grateful (0.008853276), consoling (0.008440499), neutral (0.008405817), \n",
      "\n",
      "It looks dangerous .\n",
      "agreeing, confident, encouraging, suggesting, prepared, afraid, jealous, trusting, terrified, impressed, anticipating, neutral, sad, wishing, anxious, furious, ashamed, faithful, embarrassed, lonely, disgusted, grateful, content, caring, devastated, excited, disappointed, proud, consoling, hopeful, nostalgic, acknowledging, \n",
      "agreeing (0.102942154), confident (0.09451571), encouraging (0.08537459), suggesting (0.06439759), prepared (0.05507082), afraid (0.050626244), jealous (0.039110363), trusting (0.03754214), terrified (0.035501905), impressed (0.03338686), anticipating (0.023667915), neutral (0.022764875), sad (0.02027118), wishing (0.019906497), anxious (0.019871557), furious (0.01724728), ashamed (0.016731521), faithful (0.016398), embarrassed (0.016323991), lonely (0.016249266), disgusted (0.013943261), grateful (0.013403096), content (0.013382455), caring (0.013257321), devastated (0.012632652), excited (0.012305825), disappointed (0.012116811), proud (0.011668648), consoling (0.011396838), hopeful (0.011261399), nostalgic (0.011088557), acknowledging (0.00938423), \n",
      "\n",
      "Whimp !\n",
      "terrified, encouraging, confident, trusting, wishing, jealous, content, afraid, anticipating, sentimental, embarrassed, prepared, lonely, grateful, hopeful, joyful, impressed, questioning, caring, consoling, angry, suggesting, proud, guilty, ashamed, disgusted, acknowledging, apprehensive, neutral, furious, devastated, agreeing, \n",
      "terrified (0.0784295), encouraging (0.07313185), confident (0.06700863), trusting (0.04854414), wishing (0.036970552), jealous (0.03591166), content (0.035903476), afraid (0.032710135), anticipating (0.031090885), sentimental (0.030967455), embarrassed (0.027771598), prepared (0.026149921), lonely (0.02570976), grateful (0.025042413), hopeful (0.024853682), joyful (0.024773499), impressed (0.024594327), questioning (0.024433266), caring (0.024200594), consoling (0.023961183), angry (0.02276833), suggesting (0.022175774), proud (0.019141626), guilty (0.018126456), ashamed (0.017775154), disgusted (0.017643828), acknowledging (0.01632805), apprehensive (0.015474814), neutral (0.0149961915), furious (0.01469391), devastated (0.014080668), agreeing (0.014063288), \n",
      "\n",
      "Uh , you see ?  If you don 't go pass that line , there 's no problem .\n",
      "trusting, prepared, confident, ashamed, disappointed, sad, anticipating, jealous, agreeing, lonely, suggesting, embarrassed, joyful, devastated, hopeful, wishing, consoling, grateful, furious, caring, annoyed, nostalgic, encouraging, terrified, guilty, sentimental, neutral, afraid, excited, apprehensive, impressed, disgusted, \n",
      "trusting (0.09388085), prepared (0.07130647), confident (0.06696549), ashamed (0.058518264), disappointed (0.049643934), sad (0.048673578), anticipating (0.044329755), jealous (0.043777198), agreeing (0.0406384), lonely (0.03391724), suggesting (0.03126104), embarrassed (0.031022545), joyful (0.02671257), devastated (0.02633523), hopeful (0.024191985), wishing (0.023637896), consoling (0.022679554), grateful (0.022529399), furious (0.02247679), caring (0.021295898), annoyed (0.020492535), nostalgic (0.018112399), encouraging (0.014740671), terrified (0.013523082), guilty (0.01277471), sentimental (0.012590336), neutral (0.009329952), afraid (0.009165639), excited (0.00915375), apprehensive (0.008929916), impressed (0.007913786), disgusted (0.0076516913), \n",
      "\n",
      "Still dangerous .  I think we better go home .\n",
      "encouraging, agreeing, afraid, confident, prepared, devastated, trusting, apprehensive, impressed, nostalgic, wishing, jealous, ashamed, hopeful, suggesting, grateful, furious, neutral, content, terrified, joyful, anxious, sentimental, excited, proud, annoyed, questioning, consoling, acknowledging, guilty, caring, disappointed, \n",
      "encouraging (0.13319956), agreeing (0.083263054), afraid (0.058672067), confident (0.05327108), prepared (0.043728154), devastated (0.041399248), trusting (0.035587627), apprehensive (0.03347805), impressed (0.030729474), nostalgic (0.030486854), wishing (0.027097752), jealous (0.026508395), ashamed (0.026039852), hopeful (0.022559512), suggesting (0.022462575), grateful (0.021541925), furious (0.017913904), neutral (0.01748659), content (0.017328823), terrified (0.017225718), joyful (0.016918259), anxious (0.015972592), sentimental (0.015883174), excited (0.014229349), proud (0.012745966), annoyed (0.012681272), questioning (0.012536859), consoling (0.012123471), acknowledging (0.011651317), guilty (0.011599128), caring (0.011312207), disappointed (0.011166368), \n",
      "\n",
      "You can 't .  Once you 've come in here , you cannot leave .  You must show your courage first .\n",
      "confident, prepared, agreeing, trusting, suggesting, ashamed, apprehensive, furious, jealous, devastated, sentimental, disappointed, sad, anticipating, caring, nostalgic, terrified, joyful, lonely, consoling, guilty, wishing, annoyed, hopeful, encouraging, angry, embarrassed, impressed, content, afraid, sympathizing, excited, \n",
      "confident (0.111918494), prepared (0.058393992), agreeing (0.050438933), trusting (0.04941799), suggesting (0.045263927), ashamed (0.04486092), apprehensive (0.04455133), furious (0.041330326), jealous (0.040412474), devastated (0.036779694), sentimental (0.033118423), disappointed (0.027771542), sad (0.025431843), anticipating (0.024760362), caring (0.023484116), nostalgic (0.022608332), terrified (0.022517772), joyful (0.0220891), lonely (0.01962316), consoling (0.019486666), guilty (0.018812485), wishing (0.01872458), annoyed (0.018163774), hopeful (0.016110146), encouraging (0.014741123), angry (0.014202437), embarrassed (0.012777029), impressed (0.012315569), content (0.011747142), afraid (0.01118777), sympathizing (0.01102863), excited (0.010930805), \n",
      "\n",
      "And why do we have to test it ?\n",
      "confident, prepared, afraid, agreeing, trusting, ashamed, impressed, encouraging, embarrassed, devastated, terrified, suggesting, jealous, apprehensive, consoling, wishing, anticipating, guilty, angry, acknowledging, nostalgic, content, caring, joyful, sentimental, disgusted, proud, lonely, sad, disappointed, faithful, anxious, \n",
      "confident (0.08757593), prepared (0.06282188), afraid (0.051365018), agreeing (0.050766103), trusting (0.050130054), ashamed (0.04951749), impressed (0.042367324), encouraging (0.036884356), embarrassed (0.03181686), devastated (0.030584127), terrified (0.029435433), suggesting (0.027117502), jealous (0.025995838), apprehensive (0.023984563), consoling (0.023341788), wishing (0.022831751), anticipating (0.021793531), guilty (0.021636197), angry (0.02110594), acknowledging (0.02063127), nostalgic (0.019989423), content (0.019650133), caring (0.018870477), joyful (0.017465306), sentimental (0.016801074), disgusted (0.01646285), proud (0.016093416), lonely (0.014918235), sad (0.014084048), disappointed (0.014061378), faithful (0.012702613), anxious (0.012341934), \n",
      "\n",
      "Because you are Rusty .  And rusty kids are losers .\n",
      "confident, agreeing, prepared, suggesting, trusting, content, jealous, encouraging, afraid, impressed, hopeful, terrified, wishing, devastated, ashamed, caring, apprehensive, nostalgic, sad, furious, anticipating, anxious, lonely, grateful, guilty, acknowledging, excited, neutral, sentimental, embarrassed, joyful, disappointed, \n",
      "confident (0.087445736), agreeing (0.07615394), prepared (0.05331767), suggesting (0.052298866), trusting (0.046885725), content (0.04394267), jealous (0.04298211), encouraging (0.041634135), afraid (0.04134003), impressed (0.032470763), hopeful (0.029824607), terrified (0.02834734), wishing (0.02692619), devastated (0.026757296), ashamed (0.02294086), caring (0.021390202), apprehensive (0.02121221), nostalgic (0.020487724), sad (0.019110335), furious (0.01692553), anticipating (0.016396118), anxious (0.016250996), lonely (0.015702413), grateful (0.015584865), guilty (0.015103347), acknowledging (0.014385504), excited (0.01435637), neutral (0.014273581), sentimental (0.012201277), embarrassed (0.011542165), joyful (0.011092429), disappointed (0.010749341), \n",
      "\n",
      "And do you dare to ?\n",
      "trusting, wishing, agreeing, caring, impressed, confident, afraid, ashamed, hopeful, joyful, embarrassed, consoling, suggesting, anticipating, faithful, proud, sentimental, annoyed, apprehensive, excited, sad, terrified, neutral, lonely, nostalgic, devastated, jealous, disappointed, prepared, angry, content, guilty, \n",
      "trusting (0.08931915), wishing (0.086929075), agreeing (0.06605476), caring (0.054998726), impressed (0.05067878), confident (0.047441695), afraid (0.04281122), ashamed (0.035953987), hopeful (0.03447178), joyful (0.03433587), embarrassed (0.030485084), consoling (0.025155222), suggesting (0.02376235), anticipating (0.023493588), faithful (0.023024939), proud (0.021518694), sentimental (0.02119115), annoyed (0.020648856), apprehensive (0.020343706), excited (0.018758448), sad (0.018658614), terrified (0.017567826), neutral (0.017278815), lonely (0.017059162), nostalgic (0.014368046), devastated (0.013917784), jealous (0.012977523), disappointed (0.012754731), prepared (0.0125628095), angry (0.0114842905), content (0.01120787), guilty (0.010303685), \n",
      "\n",
      "I do .  In that case , we go together .  Whoever gets closer , that 's the winner , and the loser must call the winner , \" Master \" .  Got it .  I think we should go be brave somewhere else .\n",
      "confident, jealous, ashamed, trusting, agreeing, encouraging, afraid, nostalgic, suggesting, impressed, furious, sad, devastated, caring, hopeful, disgusted, apprehensive, lonely, prepared, acknowledging, joyful, proud, anticipating, disappointed, annoyed, guilty, terrified, content, wishing, anxious, grateful, questioning, \n",
      "confident (0.14336), jealous (0.07238), ashamed (0.06421899), trusting (0.052534208), agreeing (0.049583714), encouraging (0.04004705), afraid (0.028790345), nostalgic (0.026916966), suggesting (0.026612032), impressed (0.024861194), furious (0.02410055), sad (0.024020823), devastated (0.022965923), caring (0.022939634), hopeful (0.021686258), disgusted (0.02126267), apprehensive (0.02043411), lonely (0.020171177), prepared (0.019547243), acknowledging (0.018796792), joyful (0.018705836), proud (0.016655743), anticipating (0.016543137), disappointed (0.015882263), annoyed (0.01586746), guilty (0.015778344), terrified (0.0151604), content (0.0146478275), wishing (0.014470211), anxious (0.014122509), grateful (0.013835143), questioning (0.013457483), \n",
      "\n",
      "You have to come with us .\n",
      "confident, sentimental, encouraging, embarrassed, jealous, impressed, prepared, terrified, content, anticipating, angry, neutral, trusting, sympathizing, devastated, faithful, agreeing, wishing, consoling, annoyed, suggesting, anxious, apprehensive, hopeful, furious, proud, afraid, sad, guilty, ashamed, joyful, caring, \n",
      "confident (0.060156982), sentimental (0.05105253), encouraging (0.043973718), embarrassed (0.042227723), jealous (0.041650508), impressed (0.037078913), prepared (0.035910487), terrified (0.035677858), content (0.03549116), anticipating (0.033434518), angry (0.03272983), neutral (0.030401265), trusting (0.02878479), sympathizing (0.02856238), devastated (0.027492959), faithful (0.0258142), agreeing (0.025160845), wishing (0.024170183), consoling (0.0241185), annoyed (0.022426415), suggesting (0.021145256), anxious (0.02110985), apprehensive (0.020218877), hopeful (0.02019474), furious (0.020095252), proud (0.02007106), afraid (0.019984929), sad (0.019886982), guilty (0.017370882), ashamed (0.015783558), joyful (0.015693175), caring (0.015185496), \n",
      "\n",
      "I don 't want to be anyone 's master .\n",
      "prepared, confident, ashamed, agreeing, jealous, encouraging, suggesting, trusting, anticipating, content, neutral, caring, wishing, devastated, sad, hopeful, sentimental, excited, terrified, lonely, afraid, impressed, furious, annoyed, consoling, acknowledging, sympathizing, guilty, embarrassed, anxious, nostalgic, apprehensive, \n",
      "prepared (0.10096643), confident (0.07652912), ashamed (0.062649205), agreeing (0.05291854), jealous (0.04850185), encouraging (0.04626495), suggesting (0.0387678), trusting (0.034893434), anticipating (0.032500535), content (0.030593831), neutral (0.027136521), caring (0.025580285), wishing (0.025412353), devastated (0.022804316), sad (0.022486404), hopeful (0.021032974), sentimental (0.02084442), excited (0.01975343), terrified (0.019737137), lonely (0.019613506), afraid (0.019274417), impressed (0.018944373), furious (0.018031146), annoyed (0.016564324), consoling (0.015147617), acknowledging (0.014881056), sympathizing (0.014294863), guilty (0.013490847), embarrassed (0.013257138), anxious (0.012507513), nostalgic (0.012388555), apprehensive (0.011551684), \n",
      "\n",
      "You sissy .\n",
      "confident, jealous, suggesting, agreeing, encouraging, terrified, trusting, guilty, anticipating, impressed, excited, nostalgic, ashamed, acknowledging, prepared, embarrassed, furious, sad, hopeful, caring, content, wishing, faithful, devastated, sentimental, grateful, joyful, disappointed, consoling, anxious, disgusted, neutral, \n",
      "confident (0.13747995), jealous (0.07364586), suggesting (0.06796245), agreeing (0.05756174), encouraging (0.051692784), terrified (0.03992382), trusting (0.033679612), guilty (0.031829476), anticipating (0.02779109), impressed (0.026394114), excited (0.025322227), nostalgic (0.022949193), ashamed (0.022814633), acknowledging (0.022694172), prepared (0.021969184), embarrassed (0.020036649), furious (0.019940905), sad (0.019769864), hopeful (0.01937307), caring (0.018042801), content (0.018017769), wishing (0.017758), faithful (0.01709098), devastated (0.01570241), sentimental (0.015466802), grateful (0.014997761), joyful (0.013974008), disappointed (0.0136623895), consoling (0.011991282), anxious (0.01184629), disgusted (0.011653117), neutral (0.01090183), \n",
      "\n",
      "Hey !  Are you cheating ?\n",
      "confident, jealous, sentimental, encouraging, trusting, terrified, hopeful, afraid, prepared, neutral, embarrassed, agreeing, devastated, excited, suggesting, anticipating, impressed, lonely, wishing, apprehensive, joyful, proud, disappointed, angry, anxious, nostalgic, annoyed, faithful, content, ashamed, guilty, disgusted, \n",
      "confident (0.09452528), jealous (0.06240555), sentimental (0.05233724), encouraging (0.048052784), trusting (0.046960127), terrified (0.04144074), hopeful (0.036466405), afraid (0.035044655), prepared (0.0335168), neutral (0.0321942), embarrassed (0.031835016), agreeing (0.031176198), devastated (0.02867335), excited (0.027670607), suggesting (0.027130254), anticipating (0.024908267), impressed (0.024650339), lonely (0.023370344), wishing (0.021017144), apprehensive (0.02013473), joyful (0.018264879), proud (0.016443938), disappointed (0.015561513), angry (0.015137638), anxious (0.014391442), nostalgic (0.014127127), annoyed (0.013897639), faithful (0.013854397), content (0.0137703), ashamed (0.01363621), guilty (0.013048546), disgusted (0.012915465), \n",
      "\n",
      "I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer , closer , closer ...\n",
      "jealous, devastated, trusting, confident, furious, encouraging, sad, anxious, nostalgic, proud, apprehensive, agreeing, neutral, guilty, afraid, annoyed, hopeful, anticipating, suggesting, ashamed, caring, disgusted, prepared, content, questioning, lonely, joyful, disappointed, terrified, sentimental, angry, embarrassed, \n",
      "jealous (0.21268466), devastated (0.039884616), trusting (0.03919959), confident (0.03588692), furious (0.035270493), encouraging (0.034593057), sad (0.034392346), anxious (0.030364892), nostalgic (0.029068252), proud (0.028429344), apprehensive (0.02506582), agreeing (0.02430626), neutral (0.024174295), guilty (0.023246601), afraid (0.022658933), annoyed (0.021807605), hopeful (0.0216618), anticipating (0.021009317), suggesting (0.019535648), ashamed (0.018927267), caring (0.018341953), disgusted (0.018110057), prepared (0.017577961), content (0.016488818), questioning (0.014603918), lonely (0.014409293), joyful (0.014371711), disappointed (0.013845302), terrified (0.013747176), sentimental (0.013677363), angry (0.012462396), embarrassed (0.012189912), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialog = [\"I 've got a bad feeling about this .\",\n",
    "\"You see that dividing line ?  You dare stand near that dividing line ?\",\n",
    "\"It looks dangerous .\",\n",
    "\"Whimp !\",\n",
    "\"Uh , you see ?  If you don 't go pass that line , there 's no problem .\",\n",
    "\"Still dangerous .  I think we better go home .\",\n",
    "\"You can 't .  Once you 've come in here , you cannot leave .  You must show your courage first .\",\n",
    "\"And why do we have to test it ?\",\n",
    "\"Because you are Rusty .  And rusty kids are losers .\",\n",
    "\"And do you dare to ?\",\n",
    "\"I do .  In that case , we go together .  Whoever gets closer , that 's the winner , and the loser must call the winner , \\\" Master \\\" .  Got it .  I think we should go be brave somewhere else .\",\n",
    "\"You have to come with us .\",\n",
    "\"I don 't want to be anyone 's master .\",\n",
    "\"You sissy .\",\n",
    "\"Hey !  Are you cheating ?\",\n",
    "\"I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer .  I 'm closer , closer , closer ...\"]\n",
    "\n",
    "\n",
    "for utterance in dialog:\n",
    "    pred = predict_emotion([utterance])\n",
    "    #print(pred)\n",
    "\n",
    "    arr = np.array(pred)\n",
    "    sorted_pred = np.sort(arr)[::-1]\n",
    "    indices = arr.argsort()[-32:][::-1]\n",
    "\n",
    "    #print(sorted_pred)\n",
    "    #print(indices)\n",
    "    label_pred = []\n",
    "    for ind in indices:\n",
    "    label_pred.append(ED_emotions[ind])\n",
    "    #print(label_pred)\n",
    "\n",
    "    print(utterance)\n",
    "    print_str = \"\"\n",
    "    print_str_2 = \"\"\n",
    "    for i in range(len(label_pred)):\n",
    "    print_str += label_pred[i] + \" (\" + str(sorted_pred[i]) + \"), \"\n",
    "    print_str_2 += label_pred[i] + \", \"\n",
    "    print(print_str_2)\n",
    "    print(print_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZz9t4UpbHgj"
   },
   "source": [
    "### Load Conversation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5Sem1gWfBzl",
    "outputId": "51512c32-5ece-4ad4-8e70-1b9f42eea67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/3b/04bf42b94a22725241b47e0256458cde11f86f97572dd824e011f1ea8b20/swifter-1.0.7.tar.gz (633kB)\n",
      "\u001b[K     |████████████████████████████████| 634kB 12.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (1.1.5)\n",
      "Collecting psutil>=5.6.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/82/56cd16a4c5f53e3e5dd7b2c30d5c803e124f218ebb644ca9c30bc907eadd/psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 24.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (2.12.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (4.41.1)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0cloudpickle>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from swifter) (7.5.1)\n",
      "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (0.7.1)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from swifter) (3.2.1)\n",
      "Collecting modin[ray]>=0.8.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/a9/ead212fa94de8f14459e22b0604df9c84ff704e986b58e70396ba47668f2/modin-0.8.2-py3-none-manylinux1_x86_64.whl (533kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 31.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (1.19.4)\n",
      "Collecting fsspec>=0.6.0; extra == \"dataframe\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 10.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Collecting partd>=0.3.10; extra == \"dataframe\"\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.0.8)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.10.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.5.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (20.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (1.15.0)\n",
      "Collecting ray>=1.0.0; extra == \"ray\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/e5/0ff593f053ff4fa2a582961272ef893c21d268d3c2c52ff1e7effd891e48/ray-1.1.0-cp36-cp36m-manylinux2014_x86_64.whl (48.5MB)\n",
      "\u001b[K     |████████████████████████████████| 48.5MB 99kB/s \n",
      "\u001b[?25hCollecting pyarrow==1.0; extra == \"ray\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/0a/a89de6d747c4698af128a46398703e3d1889f196478fd94a4e16bd1b5c65/pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2MB)\n",
      "\u001b[K     |████████████████████████████████| 17.2MB 273kB/s \n",
      "\u001b[?25hCollecting locket\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/22/3c0f97614e0be8386542facb3a7dcfc2584f7b83608c02333bced641281c/locket-0.2.0.tar.gz\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.7.0)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.5)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.18)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (51.0.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.4.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.8.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach>=3.1.1->swifter) (2.4.7)\n",
      "Collecting py-spy>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 38.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.12.4)\n",
      "Collecting aioredis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Collecting gpustat\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n",
      "\u001b[?25hCollecting opencensus\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/68/4f407bc0980158001c802222fab17e946728aef13f42e5d80d39dfc9ca67/opencensus-0.7.11-py2.py3-none-any.whl (127kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 51.4MB/s \n",
      "\u001b[?25hCollecting aiohttp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 43.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.32.0)\n",
      "Collecting aiohttp-cors\n",
      "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.9.0)\n",
      "Collecting redis>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.1.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.12)\n",
      "Collecting colorful\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 55.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.13)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (20.0.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.9.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.11.2)\n",
      "Collecting async-timeout\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting hiredis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.352.0)\n",
      "Collecting blessings>=1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Collecting opencensus-context==0.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (20.3.0)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 51.5MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 52.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.10)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.1.1)\n",
      "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.52.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.17.2)\n",
      "Collecting immutables>=0.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 13.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.4.8)\n",
      "Building wheels for collected packages: swifter, locket, gpustat, idna-ssl, contextvars\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for swifter: filename=swifter-1.0.7-cp36-none-any.whl size=13977 sha256=9c941253bde2faade43aacf9d4620771530a6ce274d8a30331de5e967270a17f\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/58/39/5b59c5f4d66ce67bf55f0178e0940c964e89e9f60d70376a37\n",
      "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for locket: filename=locket-0.2.0-cp36-none-any.whl size=4042 sha256=62bc31abb6b95ed97dae172085ccb62f63b74116e6284a0dd905a03662bd4bb7\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/1e/e8/4fa236ec931b1a0cdd61578e20d4934d7bf188858723b84698\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=1afbb852e339b2fea550f11fdb36ebe248052552e582ff620fe372540a582bd2\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=6b189b3b701401fec1bdd8f35b316c1c5201d3274762384a8a77e31b9a6ac890\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7667 sha256=69446ad9381716215a84da67d270c5844f3572cc91ce5c5721644288b05a6a7e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
      "Successfully built swifter locket gpustat idna-ssl contextvars\n",
      "\u001b[31mERROR: modin 0.8.2 has requirement pandas==1.1.4, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: psutil, py-spy, async-timeout, hiredis, aioredis, colorama, blessings, gpustat, immutables, contextvars, opencensus-context, opencensus, idna-ssl, multidict, yarl, aiohttp, aiohttp-cors, redis, colorful, ray, pyarrow, modin, swifter, fsspec, locket, partd\n",
      "  Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "  Found existing installation: pyarrow 0.14.1\n",
      "    Uninstalling pyarrow-0.14.1:\n",
      "      Successfully uninstalled pyarrow-0.14.1\n",
      "Successfully installed aiohttp-3.7.3 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 fsspec-0.8.5 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 locket-0.2.0 modin-0.8.2 multidict-5.1.0 opencensus-0.7.11 opencensus-context-0.1.2 partd-1.1.0 psutil-5.8.0 py-spy-0.3.3 pyarrow-1.0.0 ray-1.1.0 redis-3.5.3 swifter-1.0.7 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXQEdCsHbtT6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocess as mp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWZRFvzJTIEx"
   },
   "outputs": [],
   "source": [
    "# load dyadic data\n",
    "sad_dyadic_convs_clean = pd.read_csv('sad_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depression_dyadic_convs_clean = pd.read_csv('depression_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depressed_dyadic_convs_clean = pd.read_csv('depressed_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "offmychest_dyadic_convs_clean = pd.read_csv('offmychest_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "suicidewatch_dyadic_convs_clean = pd.read_csv('suicidewatch_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depression_help_dyadic_convs_clean = pd.read_csv('depression_help_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "anxietyhelp_dyadic_convs_clean = pd.read_csv('anxietyhelp_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "mentalhealthsupport_dyadic_convs_clean = pd.read_csv('mentalhealthsupport_dyadic_convs_clean_sentiment.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9u78CYMQUgx"
   },
   "outputs": [],
   "source": [
    "# load multiparty data\n",
    "sad_multi_convs_clean = pd.read_csv('sad_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depression_multi_convs_clean = pd.read_csv('depression_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depressed_multi_convs_clean = pd.read_csv('depressed_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "offmychest_multi_convs_clean = pd.read_csv('offmychest_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "suicidewatch_multi_convs_clean = pd.read_csv('suicidewatch_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "depression_help_multi_convs_clean = pd.read_csv('depression_help_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "anxietyhelp_multi_convs_clean = pd.read_csv('anxietyhelp_multi_convs_clean_sentiment.csv', lineterminator='\\n')\n",
    "mentalhealthsupport_multi_convs_clean = pd.read_csv('mentalhealthsupport_multi_convs_clean_sentiment.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GeWbat_dyXE"
   },
   "outputs": [],
   "source": [
    "def emobert_predict_emotion(text):\n",
    "\n",
    "    # predict emotion using emoBERT model\n",
    "    pred = predict_emotion([text])\n",
    "\n",
    "    # catch the emotion label having the largest prediction probability\n",
    "    arr = np.array(pred)\n",
    "    sorted_pred = np.sort(arr)[::-1]\n",
    "    indices = arr.argsort()[-32:][::-1]\n",
    "    emotion_pred = ED_emotions[indices[0]]\n",
    "\n",
    "    return emotion_pred\n",
    "\n",
    "def apply_parallel(grouped_df, func):\n",
    "    retLst = Parallel(n_jobs = mp.cpu_count())(delayed(func)(group) for id, group in grouped_df)\n",
    "    return pd.concat(retLst)\n",
    "\n",
    "def emobert_predict_emotion_df(df_convs):\n",
    "    \n",
    "    # apply emotion prediction function on the text column in dataframe\n",
    "    df_convs['text'] = df_convs['text'].astype(str)\n",
    "    df_convs['emotion prediction'] = df_convs['text'].swifter.apply(emobert_predict_emotion)\n",
    "\n",
    "    return df_convs\n",
    "\n",
    "def emobert_predict_emotion_df_parallel(df_convs):\n",
    "    df_convs_emotion = apply_parallel(df_convs.groupby(df_convs['conversation id']), emobert_predict_emotion_df)\n",
    "\n",
    "    return df_convs_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-mbNc8lgXEP"
   },
   "outputs": [],
   "source": [
    "# emotion prediction (dyadic)\n",
    "sad_dyadic_convs_clean_emotion = emobert_predict_emotion_df(sad_dyadic_convs_clean)\n",
    "depression_dyadic_convs_clean_emotion = emobert_predict_emotion_df(depression_dyadic_convs_clean)\n",
    "depressed_dyadic_convs_clean_emotion = emobert_predict_emotion_df(depressed_dyadic_convs_clean)\n",
    "offmychest_dyadic_convs_clean_emotion = emobert_predict_emotion_df(offmychest_dyadic_convs_clean)\n",
    "suicidewatch_dyadic_convs_clean_emotion = emobert_predict_emotion_df(suicidewatch_dyadic_convs_clean)\n",
    "depression_help_dyadic_convs_clean_emotion = emobert_predict_emotion_df(depression_help_dyadic_convs_clean)\n",
    "anxietyhelp_dyadic_convs_clean_emotion = emobert_predict_emotion_df(anxietyhelp_dyadic_convs_clean)\n",
    "mentalhealthsupport_dyadic_convs_clean_emotion = emobert_predict_emotion_df(mentalhealthsupport_dyadic_convs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjttMeDrRCnW"
   },
   "outputs": [],
   "source": [
    "# emotion prediction (multiparty)\n",
    "sad_multi_convs_clean_emotion = emobert_predict_emotion_df(sad_multi_convs_clean)\n",
    "depression_multi_convs_clean_emotion = emobert_predict_emotion_df(depression_multi_convs_clean)\n",
    "depressed_multi_convs_clean_emotion = emobert_predict_emotion_df(depressed_multi_convs_clean)\n",
    "offmychest_multi_convs_clean_emotion = emobert_predict_emotion_df(offmychest_multi_convs_clean)\n",
    "suicidewatch_multi_convs_clean_emotion = emobert_predict_emotion_df(suicidewatch_multi_convs_clean)\n",
    "depression_help_multi_convs_clean_emotion = emobert_predict_emotion_df(depression_help_multi_convs_clean)\n",
    "anxietyhelp_multi_convs_clean_emotion = emobert_predict_emotion_df(anxietyhelp_multi_convs_clean)\n",
    "mentalhealthsupport_multi_convs_clean_emotion = emobert_predict_emotion_df(mentalhealthsupport_multi_convs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pUnFYl6mnnT"
   },
   "outputs": [],
   "source": [
    "# save (dyadic)\n",
    "sad_dyadic_convs_clean_emotion.to_csv('sad_dyadic_convs_clean_emotion.csv', index = False)\n",
    "depression_dyadic_convs_clean_emotion.to_csv('depression_dyadic_convs_clean_emotion.csv', index = False)\n",
    "depressed_dyadic_convs_clean_emotion.to_csv('depressed_dyadic_convs_clean_emotion.csv', index = False)\n",
    "offmychest_dyadic_convs_clean_emotion.to_csv('offmychest_dyadic_convs_clean_emotion.csv', index = False)\n",
    "suicidewatch_dyadic_convs_clean_emotion.to_csv('suicidewatch_dyadic_convs_clean_emotion.csv', index = False)\n",
    "depression_help_dyadic_convs_clean_emotion.to_csv('depression_help_dyadic_convs_clean_emotion.csv', index = False)\n",
    "anxietyhelp_dyadic_convs_clean_emotion.to_csv('anxietyhelp_dyadic_convs_clean_emotion.csv', index = False)\n",
    "mentalhealthsupport_dyadic_convs_clean_emotion.to_csv('mentalhealthsupport_dyadic_convs_clean_emotion.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljg-VXs0RYaC"
   },
   "outputs": [],
   "source": [
    "# save (multiparty)\n",
    "sad_multi_convs_clean_emotion.to_csv('sad_multi_convs_clean_emotion.csv', index = False)\n",
    "depression_multi_convs_clean_emotion.to_csv('depression_multi_convs_clean_emotion.csv', index = False)\n",
    "depressed_multi_convs_clean_emotion.to_csv('depressed_multi_convs_clean_emotion.csv', index = False)\n",
    "offmychest_multi_convs_clean_emotion.to_csv('offmychest_multi_convs_clean_emotion.csv', index = False)\n",
    "suicidewatch_multi_convs_clean_emotion.to_csv('suicidewatch_multi_convs_clean_emotion.csv', index = False)\n",
    "depression_help_multi_convs_clean_emotion.to_csv('depression_help_multi_convs_clean_emotion.csv', index = False)\n",
    "anxietyhelp_multi_convs_clean_emotion.to_csv('anxietyhelp_multi_convs_clean_emotion.csv', index = False)\n",
    "mentalhealthsupport_multi_convs_clean_emotion.to_csv('mentalhealthsupport_multi_convs_clean_emotion.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EmoBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
